# CS 625 Student Use of ChatGPT and Other Generative AI Tools

(*Adapted from guidelines developed by Dr. Courson, ODU Dept of Chemistry*)

Some AI platforms:

* ChatGPT by Open AI (powered by GPT-3.5 Turbo): https://chat.openai.com
* CoPilot by Open AI and Microsoft (powered by GPT-4 Turbo): https://copilot.microsoft.com/
* Gemini by Google: https://gemini.google.com/

## Tool Limitations and Safe Usage (Important!)

* The data these systems was trained on is not current. For example, ChatGPT's training data cuts off in Sept 2021, so it cannot discuss current events.
* These systems have human trainers, so the biases of those trainers will appear in the outputs.
* These systems are not trained to say "I don't know" so they will usually try to give you an answer. What they are actually doing is a complex version of word association, so sometimes they will generate nonsense answers, but they will say these wrong answers with the same confident style as correct answers. This doesn't happen frequently in textbook knowledge areas but in specialist areas it happens a lot, so it is important to verify outputs before accepting them as true. The term used for this behavior is "hallucinations".
* The ChatGPT base model, and most models, cannot generate citations/references. If you ask for them, the systems will give you text that looks like real citations, but they are hallucinations. Search for those references in [Google Scholar](https://scholar.google.com/) to verify. Gemini can give some links, but their outputs for references should not be trusted without verification.
* Answers are not vetted, so the user must take on the responsibility of vetting information. You can think about the answers you get as "What would an answer to the prompt *sound like*?". Remember that incorrect answers will be presented in the same manner as correct ones. AI tools can be confidently wrong.
* These systems are poor at computation and logical reasoning, so again, be careful when presenting queries that require these skills.
* If you give one of these systems a programming problem, do not blindly trust the answer. These systems do not understand the code that they are providing, so it is not guaranteed to work properly.
* *Do not give them personal information*, as it may be recorded and used in future model training.
* The companies claim no responsibility for the outputs generated, so if you use an output and it gets you into trouble, you have no recourse with them. Similarly, they use indemnity clauses in their user agreements, so if you generate content which results in legal action being brought against the company, you are on the hook for their legal fees.
* As with any tool, users must monitor their own usage and engagement. The possibility of developing dependency on these systems, as with other technologies like cell phones, is real. These tools do not replace learning, they augment it. These tools are not alive, even though the responses seem lifelike. As such, they are not your friend, nor are they a replacement for human interaction.

## Effective Prompting 101

* These systems are multilingual, so if you speak more than one language, try prompts in different languages to see which gives the most useful output.
* Clear and concise communication is best. Just like with talking to people, longer inputs lead to more confusion. 
* Including pertinent details will generate better responses.
* Chats have memory, so you can have a conversation. Responses will improve as you ask for refinements.
* Ask the model to help you with spelling and grammar checks.
* Telling the models to "Act as..." and assigning them a role will often improve outputs as it helps them tap into the right part of their data.  
* "Teach me..." will engage a step-by-step model.
* Ask the model to debate a position with you.  Example: "Let's debate about if 3D pie charts should ever be used. You take the pro position."
* If an answer is too complicated, ask it to simplify, for example, "Explain it to me like I was a child/in 8th grade/a novice."

Below is an example of an effective way to use an AI tool as a study buddy. 

Prompt (fill in [ ] with your desired content):  
`Act as a college [computer science] professor. I am a student trying to understand and learn about [preattentive processing]. Please ask me a question. I will attempt to answer it. If I am right, tell me so, then ask me a harder question. If I am wrong, tell me the right answer, give me a brief explanation of why that answer is correct, and then ask me a slightly easier question. If you understand, say â€œI am ready to help you learn" then ask me your first question.`

Be aware, however, that your professor might not always agree with the answers that the AI tool provides. So, make sure to check anything you get back with your course notes.
